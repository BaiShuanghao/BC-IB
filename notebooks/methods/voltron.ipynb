{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import voltron\n",
    "from voltron import instantiate_extractor, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v-cond',\n",
       " 'v-dual',\n",
       " 'v-gen',\n",
       " 'v-cond-base',\n",
       " 'r-mvp',\n",
       " 'r-r3m-vit',\n",
       " 'r-r3m-rn50']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voltron.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/root/code/BC-IB/third_party/methods/voltron-robotics/examples/verification/img/peel-carrot-initial.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VR3M(\n",
       "  (patch2embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (pre_norm_attn): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (pre_norm_mlp): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (language_reward): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       "  (lm): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a frozen R3M model\n",
    "load_path = \"/root/model/r3m/r3m-small\"\n",
    "r3m_vit_model, r3m_vit_preprocess = load(\"r-r3m-vit\", device=\"cuda\", freeze=True, load_path=load_path)\n",
    "# r3m_vit_model, r3m_vit_preprocess = load(\"r-r3m-rn50\", device=\"cuda\", freeze=True, load_path=load_path)\n",
    "r3m_vit_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "tensor([[[[ 90., 114., 139.,  ...,  32.,  20.,  15.],\n",
      "          [116., 152., 132.,  ...,  23.,  21.,  37.],\n",
      "          [129., 152., 142.,  ...,  41.,  48.,  36.],\n",
      "          ...,\n",
      "          [ 10.,  10.,  10.,  ...,  26.,  27.,  23.],\n",
      "          [ 10.,   9.,   9.,  ...,  19.,  26.,  25.],\n",
      "          [  9.,   9.,   8.,  ...,  15.,  16.,  25.]],\n",
      "\n",
      "         [[103., 128., 157.,  ...,  42.,  30.,  23.],\n",
      "          [129., 164., 151.,  ...,  31.,  29.,  45.],\n",
      "          [142., 167., 162.,  ...,  49.,  56.,  44.],\n",
      "          ...,\n",
      "          [ 10.,  10.,  10.,  ...,  25.,  26.,  21.],\n",
      "          [  9.,   9.,   9.,  ...,  18.,  25.,  24.],\n",
      "          [  8.,   8.,   8.,  ...,  15.,  15.,  24.]],\n",
      "\n",
      "         [[130., 153., 180.,  ...,  38.,  25.,  19.],\n",
      "          [153., 187., 171.,  ...,  27.,  25.,  41.],\n",
      "          [163., 187., 179.,  ...,  45.,  52.,  40.],\n",
      "          ...,\n",
      "          [  9.,   9.,   8.,  ...,  21.,  22.,  18.],\n",
      "          [  8.,   8.,   8.,  ...,  15.,  22.,  20.],\n",
      "          [  8.,   8.,   7.,  ...,  12.,  12.,  21.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Obtain & Preprocess an image =>> can be from a dataset, or camera on a robot, etc.\n",
    "#   => Feel free to add any language if you have it (Voltron models work either way!)\n",
    "img = r3m_vit_preprocess(read_image(image_path))[None, ...].to(\"cuda\")\n",
    "print(img.shape)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 384])\n"
     ]
    }
   ],
   "source": [
    "# Extract both multimodal AND vision-only embeddings!\n",
    "img = torch.cat((img, img), dim=0)\n",
    "r3m_vit_embeddings = r3m_vit_model.get_representations(img)\n",
    "print(r3m_vit_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "# configure a vector extractor, then\n",
    "# Use the `vector_extractor` to output dense vector representations for downstream applications!\n",
    "#   => Pass this representation to model of your choice (object detector, control policy, etc.)\n",
    "r3m_vit_vector_extractor = instantiate_extractor(r3m_vit_model)()\n",
    "r3m_vit_vector_extractor = r3m_vit_vector_extractor.to(\"cuda\")\n",
    "r3m_vit_representation = r3m_vit_vector_extractor(r3m_vit_embeddings)\n",
    "print(r3m_vit_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a frozen MVP model\n",
    "load_path = \"/root/model/mvp/mvp-small\"\n",
    "mvp_model, mvp_preprocess = load(\"r-mvp\", device=\"cuda\", freeze=True, load_path=load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5767, -0.1657,  0.2624,  ..., -1.5699, -1.7754, -1.8610],\n",
      "          [-0.1314,  0.4851,  0.1426,  ..., -1.7240, -1.7583, -1.4843],\n",
      "          [ 0.0912,  0.4851,  0.3138,  ..., -1.4158, -1.2959, -1.5014],\n",
      "          ...,\n",
      "          [-1.9467, -1.9467, -1.9467,  ..., -1.6727, -1.6555, -1.7240],\n",
      "          [-1.9467, -1.9638, -1.9638,  ..., -1.7925, -1.6727, -1.6898],\n",
      "          [-1.9638, -1.9638, -1.9809,  ..., -1.8610, -1.8439, -1.6898]],\n",
      "\n",
      "         [[-0.2325,  0.2052,  0.7129,  ..., -1.3004, -1.5105, -1.6331],\n",
      "          [ 0.2227,  0.8354,  0.6078,  ..., -1.4930, -1.5280, -1.2479],\n",
      "          [ 0.4503,  0.8880,  0.8004,  ..., -1.1779, -1.0553, -1.2654],\n",
      "          ...,\n",
      "          [-1.8606, -1.8606, -1.8606,  ..., -1.5980, -1.5805, -1.6681],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -1.7206, -1.5980, -1.6155],\n",
      "          [-1.8957, -1.8957, -1.8957,  ..., -1.7731, -1.7731, -1.6155]],\n",
      "\n",
      "         [[ 0.4614,  0.8622,  1.3328,  ..., -1.1421, -1.3687, -1.4733],\n",
      "          [ 0.8622,  1.4548,  1.1759,  ..., -1.3339, -1.3687, -1.0898],\n",
      "          [ 1.0365,  1.4548,  1.3154,  ..., -1.0201, -0.8981, -1.1073],\n",
      "          ...,\n",
      "          [-1.6476, -1.6476, -1.6650,  ..., -1.4384, -1.4210, -1.4907],\n",
      "          [-1.6650, -1.6650, -1.6650,  ..., -1.5430, -1.4210, -1.4559],\n",
      "          [-1.6650, -1.6650, -1.6824,  ..., -1.5953, -1.5953, -1.4384]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "img = mvp_preprocess(read_image(image_path))[None, ...].to(\"cuda\")\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 384])\n"
     ]
    }
   ],
   "source": [
    "mvp_embeddings = mvp_model.get_representations(img)\n",
    "print(mvp_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "mvp_vector_extractor = instantiate_extractor(mvp_model)()\n",
    "mvp_vector_extractor = mvp_vector_extractor.to(\"cuda\")\n",
    "mvp_representation = mvp_vector_extractor(mvp_embeddings)\n",
    "print(mvp_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voltron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a frozen MVP model\n",
    "load_path = \"/root/model/voltron/v-cond-small\"\n",
    "vcond_model, vcond_preprocess = load(\"v-cond\", device=\"cuda\", freeze=True, load_path=load_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = vcond_preprocess(read_image(image_path))[None, ...].to(\"cuda\")\n",
    "lang = [\"peeling a carrot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5767, -0.1657,  0.2624,  ..., -1.5699, -1.7754, -1.8610],\n",
      "          [-0.1314,  0.4851,  0.1426,  ..., -1.7240, -1.7583, -1.4843],\n",
      "          [ 0.0912,  0.4851,  0.3138,  ..., -1.4158, -1.2959, -1.5014],\n",
      "          ...,\n",
      "          [-1.9467, -1.9467, -1.9467,  ..., -1.6727, -1.6555, -1.7240],\n",
      "          [-1.9467, -1.9638, -1.9638,  ..., -1.7925, -1.6727, -1.6898],\n",
      "          [-1.9638, -1.9638, -1.9809,  ..., -1.8610, -1.8439, -1.6898]],\n",
      "\n",
      "         [[-0.2325,  0.2052,  0.7129,  ..., -1.3004, -1.5105, -1.6331],\n",
      "          [ 0.2227,  0.8354,  0.6078,  ..., -1.4930, -1.5280, -1.2479],\n",
      "          [ 0.4503,  0.8880,  0.8004,  ..., -1.1779, -1.0553, -1.2654],\n",
      "          ...,\n",
      "          [-1.8606, -1.8606, -1.8606,  ..., -1.5980, -1.5805, -1.6681],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -1.7206, -1.5980, -1.6155],\n",
      "          [-1.8957, -1.8957, -1.8957,  ..., -1.7731, -1.7731, -1.6155]],\n",
      "\n",
      "         [[ 0.4614,  0.8622,  1.3328,  ..., -1.1421, -1.3687, -1.4733],\n",
      "          [ 0.8622,  1.4548,  1.1759,  ..., -1.3339, -1.3687, -1.0898],\n",
      "          [ 1.0365,  1.4548,  1.3154,  ..., -1.0201, -0.8981, -1.1073],\n",
      "          ...,\n",
      "          [-1.6476, -1.6476, -1.6650,  ..., -1.4384, -1.4210, -1.4907],\n",
      "          [-1.6650, -1.6650, -1.6650,  ..., -1.5430, -1.4210, -1.4559],\n",
      "          [-1.6650, -1.6650, -1.6824,  ..., -1.5953, -1.5953, -1.4384]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multimodal:  torch.Size([1, 216, 384])\n",
      "visual:  torch.Size([1, 196, 384])\n"
     ]
    }
   ],
   "source": [
    "vcond_model.to(\"cuda\")\n",
    "multimodal_embeddings = vcond_model.get_representations(img, lang, mode=\"multimodal\")\n",
    "visual_embeddings = vcond_model.get_representations(img, mode=\"visual\")\n",
    "print('multimodal: ', multimodal_embeddings.shape)\n",
    "print('visual: ', visual_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n",
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "vcond_vector_extractor = instantiate_extractor(vcond_model)()\n",
    "vcond_vector_extractor = vcond_vector_extractor.to(\"cuda\")\n",
    "multimodal_representation = vcond_vector_extractor(multimodal_embeddings)\n",
    "visual_representation = vcond_vector_extractor(visual_embeddings)\n",
    "print(multimodal_representation.shape)\n",
    "print(visual_representation.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
